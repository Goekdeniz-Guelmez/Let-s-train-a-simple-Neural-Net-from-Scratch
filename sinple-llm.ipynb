{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will implement LLM from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "We will use pytorch and numpy for the AI, ML computations.\n",
    "Matplotlib for the visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gokdenizgulmez/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params\n",
    "Here are the configurations for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dimension = 8 # Also known as \"hidden_size\" in hugginface, the inner mlp dimension or \"intermediete_size\" is 8 (embeddings_dimension) * 4 = 32\n",
    "num_attention_heads = 2\n",
    "attention_head_size = embeddings_dimension // num_attention_heads # This is the dimension every attention head wil have\n",
    "num_transformer_blocks = 2\n",
    "max_context_length = 128 # also known as \"max_position_embeddings\" in hugginface, is the maximum of tokens the llm can process\n",
    "vocab_size = 0 # Set to zero initially\n",
    "eos_token = None # Define EOS token later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is only for creating the Plot to visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPlot(nums, xlabel=\"Embedding Dimensions\", title=\"Embeddings Visualization\"):\n",
    "    # Convert embeddings to numpy for visualization\n",
    "    embeddings_np = nums.detach().numpy()\n",
    "\n",
    "    # Get dimensions\n",
    "    batch_size = embeddings_np.shape[0]\n",
    "    tokenized_input_length = embeddings_np.shape[1]\n",
    "    embeddings_dimension = embeddings_np.shape[2]\n",
    "\n",
    "    # Plotting embeddings\n",
    "    fig, axes = plt.subplots(batch_size, 1, figsize=(25, 5 * batch_size), squeeze=False)  # Increase figure size for bigger squares\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        ax = axes[batch_idx, 0]\n",
    "        cax = ax.matshow(embeddings_np[batch_idx], aspect='auto', cmap='viridis')\n",
    "\n",
    "        # Add color bar for reference\n",
    "        fig.colorbar(cax, ax=ax)\n",
    "\n",
    "        # Set labels\n",
    "        ax.set_xticks(np.arange(embeddings_dimension))\n",
    "        ax.set_yticks(np.arange(tokenized_input_length))\n",
    "        ax.set_xticklabels([f'Dim {i}' for i in range(embeddings_dimension)])\n",
    "        ax.set_yticklabels([f'Token {i}' for i in range(tokenized_input_length)])\n",
    "\n",
    "        # Rotate the tick labels and set their alignment\n",
    "        plt.xticks(rotation=90)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(\"Tokens\")\n",
    "\n",
    "        # Adding numerical values to the plot\n",
    "        for i in range(tokenized_input_length):\n",
    "            for j in range(embeddings_dimension):\n",
    "                text = ax.text(j, i, f'{embeddings_np[batch_idx, i, j]:.2f}', ha='center', va='center', color='white')\n",
    "\n",
    "        ax.set_title(f\"{title} - Batch {batch_idx + 1}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def createLossPlot(epochs, losses, title=\"training\"):\n",
    "    plt.figure(dpi=500)\n",
    "    plt.plot(epochs, losses, linewidth=1)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"losses\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "Here we will create a simple Character-Level Tokenizer and the input/training text\n",
    "\n",
    "Where \"S\" is the Start of sequence SOS Token\n",
    "Where \"E\" is the End of sequence EOS Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"SThis is a input textE\" # Because its a character leverl tokenizer the SOS is: S and the EOS is: E\n",
    "\n",
    "# Here we create the Tokenizer\n",
    "chars = sorted(list(set(input_text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "tokenize = lambda s: [stoi[c] for c in s] # tokenizer: take a string, output a list of integers\n",
    "detokenize = lambda l: ''.join([itos[i] for i in l]) # detokenizer: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenize(input_text)\n",
    "\n",
    "# Convert tokenized input to a tensor and add batch dimension\n",
    "tokenized_input_tensor = torch.tensor(tokenized_input).unsqueeze(0)\n",
    "eos_token = tokenized_input[-1]\n",
    "sos_token = tokenized_input[0]\n",
    "\n",
    "print(f\"New updated vocab size: {vocab_size}\")\n",
    "print(f\"sos_token: {sos_token}\")\n",
    "print(f\"eos_token: {eos_token}\")\n",
    "print(f\"The tokenized Text (as a normal array): {tokenized_input}\")\n",
    "print(f\"The context size of the current input text is: {len(tokenized_input)}\")\n",
    "print(f\"The detokenized Tensor: \\\"{detokenize(tokenized_input)}\\\"\")\n",
    "print(f\"As you can see the space token is the number. {tokenized_input[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1: Embeddings\n",
    "\n",
    "This will give out multidimensional embeddings (a lot of numbers) that represents the given Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_dimension, padding_idx=eos_token)\n",
    "token_embeddings = embedding(torch.tensor(tokenized_input_tensor))\n",
    "\n",
    "print(f\"The shape is: {token_embeddings.shape}\")\n",
    "print(token_embeddings)\n",
    "\n",
    "print(f\"\\nFor example the Embeddings that represent the SOS token are: {token_embeddings[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Plot for the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createPlot(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 2: LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_attention_layernorm = nn.LayerNorm(embeddings_dimension)\n",
    "normalized_token_embeddings = post_attention_layernorm(token_embeddings)\n",
    "\n",
    "print(normalized_token_embeddings.shape)\n",
    "createPlot(normalized_token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 3: Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.key_proj = nn.Linear(embeddings_dimension, attention_head_size, bias=False)\n",
    "        self.query_proj = nn.Linear(embeddings_dimension, attention_head_size, bias=False)\n",
    "        self.value_proj = nn.Linear(embeddings_dimension, attention_head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(max_context_length, max_context_length)))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (Batch, Length, Dimensions)\n",
    "        # output of size (Batch, Length, head size)\n",
    "        B, L, D = x.shape\n",
    "\n",
    "        keys = self.key_proj(x)   # (B,T,hs)\n",
    "        print(\"keys beggining\")\n",
    "        createPlot(keys, xlabel=\"Created Key Projection weights\", title=\"key_proj\")\n",
    "        queries = self.query_proj(x) # (B,T,hs)\n",
    "        print(\"queries beggining\")\n",
    "        createPlot(queries, xlabel=\"Created Query Projection weights\", title=\"query_proj\")\n",
    "        values = self.value_proj(x) # (B,T,hs)\n",
    "        print(\"values beggining\")\n",
    "        createPlot(values, xlabel=\"Created Value Projection weights\", title=\"value_proj\")\n",
    "        \n",
    "        # ATTENTION START\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = queries @ keys.transpose(-2,-1) * keys.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        print(\"keys and queries multiplied\")\n",
    "        createPlot(wei, xlabel=\"Attention Beginn\", title=\"queries @ keys.T\")\n",
    "        # Mask the input\n",
    "        wei = wei.masked_fill(self.tril[:L, :L] == 0, float('-inf')) # (B, T, T)\n",
    "        print(\"Masked\")\n",
    "        createPlot(wei, xlabel=\"Masked Attention Scores\", title=\"Casual Attention\")\n",
    "\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        print(\"Softmaxed\")\n",
    "        createPlot(wei, xlabel=\"Mased Scored Softmax Probabilities\", title=\"Casual Softmax Probabilities\")\n",
    "        wei = self.dropout(wei)\n",
    "        print(\"droped out\")\n",
    "        createPlot(wei, xlabel=\"Attention Scores Droped out\", title=\"Attention Dropout\")\n",
    "        # perform the weighted aggregation of the values\n",
    "        out = wei @ values # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        print(\"Attentino head output\")\n",
    "        createPlot(out, xlabel=\"Final Attenion output weights\", title=\"Final Attention output\")\n",
    "        # ATTENTION END\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead() for _ in range(num_attention_heads)])\n",
    "        self.out_proj = nn.Linear(attention_head_size * num_attention_heads, embeddings_dimension, bias=False)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        print(\"Concatinated attention outputs\")\n",
    "        createPlot(out, xlabel=\"Concatinated Attention Heads output weights\", title=\"Concatinated Attention Heads\")\n",
    "        print(\"Final output of multihead attention\")\n",
    "        out = self.out_proj(out)\n",
    "        createPlot(out, xlabel=\"Final Multi-Head-Attenion output\", title=\"out_proj\")\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention()\n",
    "print(mha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_output = mha(normalized_token_embeddings)\n",
    "print(mha_output.shape)\n",
    "print(mha_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 4: MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gate_proj = nn.Linear(embeddings_dimension, 4 * embeddings_dimension, bias=False)\n",
    "        self.up_proj = nn.Linear(embeddings_dimension, 4 * embeddings_dimension, bias=False)\n",
    "\n",
    "        self.down_proj = nn.Linear(4 * embeddings_dimension, embeddings_dimension, bias=False)\n",
    "\n",
    "        self.act_fn = nn.SELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        print(\"gate_proj\")\n",
    "        createPlot(gate, xlabel=\"created weights in the gate_proj\", title=\"gate_proj\")\n",
    "\n",
    "        up = self.up_proj(x)\n",
    "        print(\"up_proj\")\n",
    "        createPlot(up, xlabel=\"created weights in the up_proj\", title=\"up_proj\")\n",
    "\n",
    "        x = self.act_fn(gate * up)\n",
    "        print(\"multiplied and activated\")\n",
    "        createPlot(x, xlabel=\"Matrix Multiplied and Activated weights\", title=\"self.act_fn(gate * up)\")\n",
    "\n",
    "        down = self.down_proj(x)\n",
    "        print(\"down_proj\")\n",
    "        createPlot(down, xlabel=\"Final MLP output, and created weights in the down_proj\", title=\"down_proj\")\n",
    "        return down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_output = mlp(mha_output)\n",
    "\n",
    "print(mlp_output.shape)\n",
    "print(mlp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And we're done, lets create the Blocks and everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.mlp = MLP()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embeddings_dimension)\n",
    "        self.ln2 = nn.LayerNorm(embeddings_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_o = self.attn(self.ln1(x))\n",
    "        print(\"Transformer Block attn output:\")\n",
    "        createPlot(attn_o, xlabel=\"Transformer Block attn output scores\", title=\"self.attn(self.ln1(x))\")\n",
    "\n",
    "        x = x + attn_o\n",
    "        print(\"Transformer Block first Residual Conection:\")\n",
    "        createPlot(x, xlabel=\"Transformer Block first Residual Conection scores\", title=\"x + attn_o\")\n",
    "\n",
    "        mlp_o = self.mlp(self.ln2(x))\n",
    "        print(\"Transformer Block mlp output:\")\n",
    "        createPlot(mlp_o, xlabel=\"Transformer Block mlp output scores\", title=\"self.mlp(self.ln2(x))\")\n",
    "\n",
    "        x = x + mlp_o\n",
    "        print(\"Transformer Block second Residual Conection and final Output:\")\n",
    "        createPlot(x, xlabel=\"ransformer Block second Residual Conection and final output scores\", title=\"x + mlp_o\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_block = TransformerBlock()\n",
    "print(transformer_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_block(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinusoidal positional encoding\n",
    "def get_sinusoidal_positional_encoding(seq_len, d_model):\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.positional_encoding = get_sinusoidal_positional_encoding(max_context_length, embeddings_dimension)\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerBlock() for _ in range(num_transformer_blocks)])\n",
    "        self.ln_f = nn.LayerNorm(embeddings_dimension)\n",
    "        self.lm_head = nn.Linear(embeddings_dimension, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
    "        print(f\"Positionaly Encoded Embeddings:\")\n",
    "        createPlot(x, xlabel=\"Positional Encodings\", title=\"embeddings.float() + self.positional_encoding[:, :x.size(1), :]\")\n",
    "        \n",
    "        x = self.transformer_blocks(x) # (B,T,C)\n",
    "        print(f\"output transformer blocks:\")\n",
    "        createPlot(x, xlabel=\"outputs of the transformer blocks\", title=\"self.transformer_blocks(x)\")\n",
    "\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        print(f\"layer norm:\")\n",
    "        createPlot(x, xlabel=\"layer norm of the attention scores from the blocks\", title=\"self.ln_f(x)\")\n",
    "\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        print(f\"output lm heads (logits):\")\n",
    "        createPlot(x, xlabel=\"Final output logits of the LLM\", title=\"self.lm_head(x)\")\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass the created embeddings through the Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(token_embeddings)\n",
    "# so the output of the transformer is the shape torch.Size([22, 14]) the input was torch.Size([22, 13]) the last layer in the shape is the next generated token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus only on the last time step eg the last embeddings layer\n",
    "last_logits = logits[:, -1, :] # becomes (B, D)\n",
    "\n",
    "print(f\"The LLm basicaly outputs the embeddings of the next predicted Token:\\n{last_logits.tolist()}\")\n",
    "\n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(last_logits, dim=-1) # (B, D)\n",
    "\n",
    "# sample from the distribution\n",
    "next_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detokenize the generated output Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_generated_text = detokenize([next_token.item()])\n",
    "\n",
    "print(f\"The next generated Token is: \\\"{the_generated_text}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.key_proj = nn.Linear(embeddings_dimension, attention_head_size, bias=False)\n",
    "        self.query_proj = nn.Linear(embeddings_dimension, attention_head_size, bias=False)\n",
    "        self.value_proj = nn.Linear(embeddings_dimension, attention_head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(max_context_length, max_context_length)))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B, L, D = x.shape\n",
    "\n",
    "        keys = self.key_proj(x)   # (B,T,hs)\n",
    "        queries = self.query_proj(x) # (B,T,hs)\n",
    "        values = self.value_proj(x) # (B,T,hs)\n",
    "        \n",
    "        # ATTENTION START\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = queries @ keys.transpose(-2,-1) * keys.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:L, :L] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        out = wei @ values # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        # ATTENTION END\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead() for _ in range(num_attention_heads)])\n",
    "        self.out_proj = nn.Linear(attention_head_size * num_attention_heads, embeddings_dimension, bias=False)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.out_proj(out)\n",
    "        return self.dropout(out)\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gate_proj = nn.Linear(embeddings_dimension, 4 * embeddings_dimension, bias=False)\n",
    "        self.down_proj = nn.Linear(4 * embeddings_dimension, embeddings_dimension, bias=False)\n",
    "        self.up_proj = nn.Linear(embeddings_dimension, 4 * embeddings_dimension, bias=False)\n",
    "        self.act_fn = nn.SELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x) * self.up_proj(x)))\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.mlp = MLP()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embeddings_dimension)\n",
    "        self.ln2 = nn.LayerNorm(embeddings_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_dimension, padding_idx=eos_token)\n",
    "        self.positional_encoding = get_sinusoidal_positional_encoding(max_context_length, embeddings_dimension)\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerBlock() for _ in range(num_transformer_blocks)])\n",
    "        self.ln_f = nn.LayerNorm(embeddings_dimension)\n",
    "        self.lm_head = nn.Linear(embeddings_dimension, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.embeddings(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer_blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, L, D = logits.shape\n",
    "            logits = logits.view(B*L, D)\n",
    "            targets = targets.view(B*L)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -max_context_length:]\n",
    "            new_logits, _ = self(idx_cond)\n",
    "            new_logits = new_logits[:, -1, :]\n",
    "            probs = F.softmax(new_logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = LlamaModel()\n",
    "print(sum(p.numel() for p in llama.parameters())/1e6, 'M parameters')\n",
    "print(llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llama.generate(tokenized_input_tensor, max_new_tokens=50)\n",
    "out = output[0].tolist()\n",
    "generated_text = detokenize(out)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "max_epochs = 100\n",
    "eval_interval = 10\n",
    "eval_epochs = 200\n",
    "\n",
    "# Ensure tokenized_input has sufficient length for max_context_length\n",
    "if len(tokenized_input) < max_context_length:\n",
    "    tokenized_input += [eos_token] * (max_context_length - len(tokenized_input) - 1)\n",
    "\n",
    "# Train and validation split\n",
    "data = torch.tensor(tokenized_input, dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# After that the traing dataset looks like this \"SThis is a input textEEEEEEEEEEEEEEEEEEE......\"\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, max(len(data) - max_context_length, 1), (batch_size,))\n",
    "    x = torch.stack([data[i:i+max_context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+max_context_length+1] for i in ix])\n",
    "\n",
    "    # Pad sequences if necessary\n",
    "    if x.size(1) < max_context_length:\n",
    "        pad_size = max_context_length - x.size(1)\n",
    "        pad_x = torch.full((batch_size, pad_size), eos_token, dtype=torch.long)\n",
    "        pad_y = torch.full((batch_size, pad_size), eos_token, dtype=torch.long)\n",
    "        x = torch.cat([x[:, :-1], pad_x], dim=1)\n",
    "        y = torch.cat([y, pad_y], dim=1)\n",
    "    \n",
    "    return x.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    llama.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_epochs)\n",
    "        for k in range(eval_epochs):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = llama(X, targets=Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    llama.train()\n",
    "    return out\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(llama.parameters(), lr=0.0005)\n",
    "\n",
    "for epoch in range(max_epochs - 1):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if epoch % eval_interval == 0 or epoch == max_epochs - 1:\n",
    "        losses = estimate_loss()\n",
    "        epochs.append(epoch)\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "        print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = llama(xb, targets=yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "torch.save(llama.state_dict(), \"pytorch_model.pth\")\n",
    "createLossPlot(epochs, train_losses)\n",
    "createLossPlot(epochs, val_losses, title=\"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's test our trained model\n",
    "\n",
    "The start input is \"SThis\" and we want it to complete is to the training dataset.\n",
    "\n",
    "So the expected Output should be \"SThis is a input textEEEEEEEEEEEEEEEEEEE......\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama.load_state_dict(torch.load(\"pytorch_model.pth\"))\n",
    "\n",
    "test_input_text = \"SThis\"\n",
    "test_tokenized_input = tokenize(input_text)\n",
    "test_tokenized_input_tensor = torch.tensor(tokenized_input).unsqueeze(0)\n",
    "output = llama.generate(test_tokenized_input_tensor, max_new_tokens=50)\n",
    "out = output[0].tolist()\n",
    "generated_text = detokenize(out)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
