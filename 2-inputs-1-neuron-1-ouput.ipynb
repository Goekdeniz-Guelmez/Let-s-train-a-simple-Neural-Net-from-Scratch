{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "<p>We will train a single Neuron with one input and output.</p>\n",
    "<p>For the harder calculations, we need to imoprt the math package in python.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's start by defining our input and output.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = 1\n",
    "input2 = 0\n",
    "expected_ouput = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Neuron\n",
    "\n",
    "<p>The first time a Neuron is created, it gets a randomly assigned values for its weight and bias.</p>\n",
    "<p>The number of weights is the same as the inputs and every Neuron get's one bias, you can also chose to leave the bias out. So therefore, whe need two weights and one bias.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = 0.4\n",
    "weight2 = -1.2\n",
    "bias = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propogation\n",
    "\n",
    "<p>After creating the Neuron we can go ahead and Forward Pass through the model.</p>\n",
    "<p>We are going to compute the weightes sum of the inputs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the output of the Neuron is: 1.4\n"
     ]
    }
   ],
   "source": [
    "output_neuron = input1 * weight1 + input2 * weight2 + bias\n",
    "print(f\" the output of the Neuron is: {output_neuron}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run that output through the Activation Function to finish the Forward Pass. the Functin we'll use is the Sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8021838885585817\n"
     ]
    }
   ],
   "source": [
    "activated = 1 / (1 + math.exp(-output_neuron))\n",
    "print(activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss calculation\n",
    "\n",
    "<p>Perfect! We are done with the Forward Propogation. We can now calculate the loss. The Algorythm we will be using is the \"Mean Square Error\".</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9799999999999999\n"
     ]
    }
   ],
   "source": [
    "loss = 0.5 * (output_neuron - expected_ouput) ** 2\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propogation\n",
    "\n",
    "<p>Now we got everything setup and can procede with the Backward Pass. And the first thing we need to do is to calculate the gradients.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of the loss with respect to the Neuron's output is: 1.4\n"
     ]
    }
   ],
   "source": [
    "# Gradient of the loss with respect to the Neuron's output.\n",
    "d_loss_d_output = output_neuron - expected_ouput\n",
    "print(f\"Gradient of the loss with respect to the Neuron's output is: {d_loss_d_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can use the derivative to calculate this perticular gradient.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of the NN output with respect to nn_output is: -0.5599999999999998\n"
     ]
    }
   ],
   "source": [
    "# Gradient of the NN output with respect to nn_output\n",
    "d_output_d_z = output_neuron * (1 - output_neuron)\n",
    "print(f\"Gradient of the NN output with respect to nn_output is: {d_output_d_z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of Weight1: -0.3135999999999999, gradient of Weight2: 0.9407999999999996, gradient of the bias: -0.7839999999999997\n"
     ]
    }
   ],
   "source": [
    "# Let's combine gradients to get the gradient of the loss with respect to w1, w2, and bias\n",
    "d_loss_d_w1 = d_loss_d_output * d_output_d_z * weight1\n",
    "d_loss_d_w2 = d_loss_d_output * d_output_d_z * weight2\n",
    "d_loss_d_bias = d_loss_d_output * d_output_d_z * bias\n",
    "\n",
    "print(f\"gradient of Weight1: {d_loss_d_w1}, gradient of Weight2: {d_loss_d_w2}, gradient of the bias: {d_loss_d_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
