{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "<p>Let's expand our Neuron into a Nural Network. With 2 more Neurons, Input Layer and ouput layer.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = 1\n",
    "input2 = 0\n",
    "expected_ouput = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron1_weight1 = 0.4\n",
    "neuron1_weight2 = -1.2\n",
    "neuron1_bias = 1\n",
    "\n",
    "neuron2_weight1 = 4\n",
    "neuron2_weight2 = -0.7\n",
    "neuron2_bias = 1.6\n",
    "\n",
    "neuron3_weight1 = 2\n",
    "neuron3_weight2 = 0.4\n",
    "neuron3_bias = -0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_neuron1 = input1 * neuron1_weight1 + input2 * neuron1_weight2 + neuron1_bias\n",
    "print(f\" the output of the untrained Neuron is: {output_neuron1}\")\n",
    "\n",
    "activated_neuron1_output = 1 / (1 + math.exp(-output_neuron1))\n",
    "print(activated_neuron1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_neuron2 = input1 * neuron2_weight1 + input2 * neuron2_weight2 + neuron2_bias\n",
    "print(f\" the output of the untrained Neuron is: {output_neuron2}\")\n",
    "\n",
    "activated_neuron2_output = 1 / (1 + math.exp(-output_neuron2))\n",
    "print(activated_neuron2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_neuron3 = activated_neuron1_output * neuron3_weight1 + activated_neuron2_output * neuron3_weight2 + neuron3_bias\n",
    "print(f\" the output of the untrained Neuron is: {output_neuron3}\")\n",
    "\n",
    "activated_neuron3_output = 1 / (1 + math.exp(-output_neuron3))\n",
    "print(activated_neuron3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0.5 * (activated_neuron3_output - expected_ouput) ** 2\n",
    "print(f\"The first loss is: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of the loss with respect to the NN output\n",
    "d_loss_d_output = activated_neuron3_output - expected_ouput\n",
    "print(f\"Gradient of the loss with respect to the NN output: {d_loss_d_output}\")\n",
    "\n",
    "# Gradient of the NN output with respect to activated_neuron_output\n",
    "d_output_d_z = activated_neuron3_output * (1 - activated_neuron3_output)\n",
    "print(f\"Gradient of the NN output with respect to nn_output is: {d_output_d_z}\")\n",
    "\n",
    "# Let's combine gradients to get the gradient of the loss with respect to w1, w2, and bias\n",
    "d_loss_d_w1 = loss * d_output_d_z * neuron3_weight1\n",
    "d_loss_d_w2 = loss * d_output_d_z * neuron3_weight2\n",
    "d_loss_d_bias = loss * d_output_d_z * neuron3_bias\n",
    "\n",
    "print(f\"gradient of Weight1: {d_loss_d_w1}, gradient of Weight2: {d_loss_d_w2}, gradient of the bias: {d_loss_d_bias}\")\n",
    "\n",
    "# Update weights and bias\n",
    "# We first multiply the learning rate with the gradients abouve and then subtract that value with the origional paramters.\n",
    "neuron3_weight1 -= learning_rate * d_loss_d_w1\n",
    "neuron3_weight2 -= learning_rate * d_loss_d_w2\n",
    "neuron3_bias -= learning_rate * d_loss_d_bias\n",
    "\n",
    "print(f\"New neuron3_weight1: {neuron3_weight1}, New neuron3_weight2: {neuron3_weight2}, New neuron3_bias: {neuron3_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of the loss with respect to the NN output\n",
    "d_loss_d_output = activated_neuron2_output - expected_ouput\n",
    "print(f\"Gradient of the loss with respect to the NN output: {d_loss_d_output}\")\n",
    "\n",
    "# Gradient of the NN output with respect to activated_neuron_output\n",
    "d_output_d_z = activated_neuron2_output * (1 - activated_neuron2_output)\n",
    "print(f\"Gradient of the NN output with respect to nn_output is: {d_output_d_z}\")\n",
    "\n",
    "# Let's combine gradients to get the gradient of the loss with respect to w1, w2, and bias\n",
    "d_loss_d_w1 = loss * d_output_d_z * neuron2_weight1\n",
    "d_loss_d_w2 = loss * d_output_d_z * neuron2_weight2\n",
    "d_loss_d_bias = loss * d_output_d_z * neuron2_bias\n",
    "\n",
    "print(f\"gradient of Weight1: {d_loss_d_w1}, gradient of Weight2: {d_loss_d_w2}, gradient of the bias: {d_loss_d_bias}\")\n",
    "\n",
    "# Update weights and bias\n",
    "# We first multiply the learning rate with the gradients abouve and then subtract that value with the origional paramters.\n",
    "neuron2_weight1 -= learning_rate * d_loss_d_w1\n",
    "neuron2_weight2 -= learning_rate * d_loss_d_w2\n",
    "neuron2_bias -= learning_rate * d_loss_d_bias\n",
    "\n",
    "print(f\"New neuron2_weight1: {neuron2_weight1}, New neuron2_weight2: {neuron2_weight2}, New neuron2_bias: {neuron2_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of the loss with respect to the NN output\n",
    "d_loss_d_output = activated_neuron1_output - expected_ouput\n",
    "print(f\"Gradient of the loss with respect to the NN output: {d_loss_d_output}\")\n",
    "\n",
    "# Gradient of the NN output with respect to activated_neuron_output\n",
    "d_output_d_z = activated_neuron1_output * (1 - activated_neuron1_output)\n",
    "print(f\"Gradient of the NN output with respect to nn_output is: {d_output_d_z}\")\n",
    "\n",
    "# Let's combine gradients to get the gradient of the loss with respect to w1, w2, and bias\n",
    "d_loss_d_w1 = loss * d_output_d_z * neuron1_weight1\n",
    "d_loss_d_w2 = loss * d_output_d_z * neuron1_weight2\n",
    "d_loss_d_bias = loss * d_output_d_z * neuron1_bias\n",
    "\n",
    "print(f\"gradient of Weight1: {d_loss_d_w1}, gradient of Weight2: {d_loss_d_w2}, gradient of the bias: {d_loss_d_bias}\")\n",
    "\n",
    "# Update weights and bias\n",
    "# We first multiply the learning rate with the gradients abouve and then subtract that value with the origional paramters.\n",
    "neuron1_weight1 -= learning_rate * d_loss_d_w1\n",
    "neuron1_weight2 -= learning_rate * d_loss_d_w2\n",
    "neuron1_bias -= learning_rate * d_loss_d_bias\n",
    "\n",
    "print(f\"New neuron1_weight1: {neuron2_weight1}, New neuron1_weight2: {neuron2_weight2}, New neuron1_bias: {neuron2_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
