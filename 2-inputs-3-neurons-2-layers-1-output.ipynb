{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "<p>Let's expand our Neuron into a Nural Network. With 2 more Neurons, Input Layer and ouput layer.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = 1\n",
    "input2 = 0\n",
    "expected_ouput = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron1_weight1 = 0.4\n",
    "neuron1_weight2 = -1.2\n",
    "neuron1_bias = 1\n",
    "\n",
    "neuron2_weight1 = 4\n",
    "neuron2_weight2 = -0.7\n",
    "neuron2_bias = 1.6\n",
    "\n",
    "neuron3_weight1 = 2\n",
    "neuron3_weight2 = 0.4\n",
    "neuron3_bias = -0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the output of the untrained Neuron is: 1.4\n",
      "0.8021838885585817\n"
     ]
    }
   ],
   "source": [
    "output_neuron1 = input1 * neuron1_weight1 + input2 * neuron1_weight2 + neuron1_bias\n",
    "print(f\" the output of the untrained Neuron is: {output_neuron1}\")\n",
    "\n",
    "activated_neuron1_output = 1 / (1 + math.exp(-output_neuron1))\n",
    "print(activated_neuron1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the output of the untrained Neuron is: 5.6\n",
      "0.9963157601005641\n"
     ]
    }
   ],
   "source": [
    "output_neuron2 = input1 * neuron2_weight1 + input2 * neuron2_weight2 + neuron2_bias\n",
    "print(f\" the output of the untrained Neuron is: {output_neuron2}\")\n",
    "\n",
    "activated_neuron2_output = 1 / (1 + math.exp(-output_neuron2))\n",
    "print(activated_neuron2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the output of the untrained Neuron is: 1.8028940811573892\n",
      "0.8585008646646531\n"
     ]
    }
   ],
   "source": [
    "output_neuron3 = activated_neuron1_output * neuron3_weight1 + activated_neuron2_output * neuron3_weight2 + neuron3_bias\n",
    "print(f\" the output of the untrained Neuron is: {output_neuron3}\")\n",
    "\n",
    "activated_neuron3_output = 1 / (1 + math.exp(-output_neuron3))\n",
    "print(activated_neuron3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first loss is: 0.010011002650325403\n"
     ]
    }
   ],
   "source": [
    "loss = 0.5 * (activated_neuron3_output - expected_ouput) ** 2\n",
    "print(f\"The first loss is: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of the loss with respect to the NN output: -0.14149913533534686\n",
      "Gradient of the NN output with respect to nn_output is: 0.12147713003469605\n",
      "gradient of Weight1: 0.0024322157414625317, gradient of Weight2: 0.00048644314829250634, gradient of the bias: -0.00024322157414625317\n",
      "New neuron3_weight1: 1.9997567784258536, New neuron3_weight2: 0.39995135568517076, New neuron3_bias: -0.19997567784258538\n"
     ]
    }
   ],
   "source": [
    "# Gradient of the loss with respect to the NN output\n",
    "d_loss_d_output = activated_neuron3_output - expected_ouput\n",
    "print(f\"Gradient of the loss with respect to the NN output: {d_loss_d_output}\")\n",
    "\n",
    "# Gradient of the NN output with respect to activated_neuron_output\n",
    "d_output_d_z = activated_neuron3_output * (1 - activated_neuron3_output)\n",
    "print(f\"Gradient of the NN output with respect to nn_output is: {d_output_d_z}\")\n",
    "\n",
    "# Let's combine gradients to get the gradient of the loss with respect to w1, w2, and bias\n",
    "d_loss_d_w1 = loss * d_output_d_z * neuron3_weight1\n",
    "d_loss_d_w2 = loss * d_output_d_z * neuron3_weight2\n",
    "d_loss_d_bias = loss * d_output_d_z * neuron3_bias\n",
    "\n",
    "print(f\"gradient of Weight1: {d_loss_d_w1}, gradient of Weight2: {d_loss_d_w2}, gradient of the bias: {d_loss_d_bias}\")\n",
    "\n",
    "# Update weights and bias\n",
    "# We first multiply the learning rate with the gradients abouve and then subtract that value with the origional paramters.\n",
    "neuron3_weight1 -= learning_rate * d_loss_d_w1\n",
    "neuron3_weight2 -= learning_rate * d_loss_d_w2\n",
    "neuron3_bias -= learning_rate * d_loss_d_bias\n",
    "\n",
    "print(f\"New neuron3_weight1: {neuron3_weight1}, New neuron3_weight2: {neuron3_weight2}, New neuron3_bias: {neuron3_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of the loss with respect to the NN output: -0.0036842398994358927\n",
      "Gradient of the NN output with respect to nn_output is: 0.0036706662757992974\n",
      "gradient of Weight1: 0.00014698819926194738, gradient of Weight2: -2.572293487084079e-05, gradient of the bias: 5.879527970477895e-05\n",
      "New neuron2_weight1: 3.999985301180074, New neuron2_weight2: -0.6999974277065129, New neuron2_bias: 1.5999941204720296\n"
     ]
    }
   ],
   "source": [
    "# Gradient of the loss with respect to the NN output\n",
    "d_loss_d_output = activated_neuron2_output - expected_ouput\n",
    "print(f\"Gradient of the loss with respect to the NN output: {d_loss_d_output}\")\n",
    "\n",
    "# Gradient of the NN output with respect to activated_neuron_output\n",
    "d_output_d_z = activated_neuron2_output * (1 - activated_neuron2_output)\n",
    "print(f\"Gradient of the NN output with respect to nn_output is: {d_output_d_z}\")\n",
    "\n",
    "# Let's combine gradients to get the gradient of the loss with respect to w1, w2, and bias\n",
    "d_loss_d_w1 = loss * d_output_d_z * neuron2_weight1\n",
    "d_loss_d_w2 = loss * d_output_d_z * neuron2_weight2\n",
    "d_loss_d_bias = loss * d_output_d_z * neuron2_bias\n",
    "\n",
    "print(f\"gradient of Weight1: {d_loss_d_w1}, gradient of Weight2: {d_loss_d_w2}, gradient of the bias: {d_loss_d_bias}\")\n",
    "\n",
    "# Update weights and bias\n",
    "# We first multiply the learning rate with the gradients abouve and then subtract that value with the origional paramters.\n",
    "neuron2_weight1 -= learning_rate * d_loss_d_w1\n",
    "neuron2_weight2 -= learning_rate * d_loss_d_w2\n",
    "neuron2_bias -= learning_rate * d_loss_d_bias\n",
    "\n",
    "print(f\"New neuron2_weight1: {neuron2_weight1}, New neuron2_weight2: {neuron2_weight2}, New neuron2_bias: {neuron2_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of the loss with respect to the NN output: -0.1978161114414183\n",
      "Gradient of the NN output with respect to nn_output is: 0.15868489749561468\n",
      "gradient of Weight1: 0.0006354379717580854, gradient of Weight2: -0.0019063139152742559, gradient of the bias: 0.0015885949293952134\n",
      "New neuron1_weight1: 0.3999364562028242, New neuron1_weight2: -1.1998093686084725, New neuron1_bias: 0.9998411405070605\n"
     ]
    }
   ],
   "source": [
    "# Gradient of the loss with respect to the NN output\n",
    "d_loss_d_output = activated_neuron1_output - expected_ouput\n",
    "print(f\"Gradient of the loss with respect to the NN output: {d_loss_d_output}\")\n",
    "\n",
    "# Gradient of the NN output with respect to activated_neuron_output\n",
    "d_output_d_z = activated_neuron1_output * (1 - activated_neuron1_output)\n",
    "print(f\"Gradient of the NN output with respect to nn_output is: {d_output_d_z}\")\n",
    "\n",
    "# Let's combine gradients to get the gradient of the loss with respect to w1, w2, and bias\n",
    "d_loss_d_w1 = loss * d_output_d_z * neuron1_weight1\n",
    "d_loss_d_w2 = loss * d_output_d_z * neuron1_weight2\n",
    "d_loss_d_bias = loss * d_output_d_z * neuron1_bias\n",
    "\n",
    "print(f\"gradient of Weight1: {d_loss_d_w1}, gradient of Weight2: {d_loss_d_w2}, gradient of the bias: {d_loss_d_bias}\")\n",
    "\n",
    "# Update weights and bias\n",
    "# We first multiply the learning rate with the gradients abouve and then subtract that value with the origional paramters.\n",
    "neuron1_weight1 -= learning_rate * d_loss_d_w1\n",
    "neuron1_weight2 -= learning_rate * d_loss_d_w2\n",
    "neuron1_bias -= learning_rate * d_loss_d_bias\n",
    "\n",
    "print(f\"New neuron1_weight1: {neuron1_weight1}, New neuron1_weight2: {neuron1_weight2}, New neuron1_bias: {neuron1_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the output of the untrained Neuron is: 1.3997775967098847\n",
      "0.8021485941434118\n",
      " the output of the untrained Neuron is: 5.599979421652104\n",
      "0.9963156845635448\n",
      " the output of the untrained Neuron is: 1.8026042193320602\n",
      "0.858465649422835\n"
     ]
    }
   ],
   "source": [
    "new_output_neuron1 = input1 * neuron1_weight1 + input2 * neuron1_weight2 + neuron1_bias\n",
    "print(f\" the output of the untrained Neuron is: {new_output_neuron1}\")\n",
    "\n",
    "new_activated_neuron1_output = 1 / (1 + math.exp(-new_output_neuron1))\n",
    "print(new_activated_neuron1_output)\n",
    "\n",
    "new_output_neuron2 = input1 * neuron2_weight1 + input2 * neuron2_weight2 + neuron2_bias\n",
    "print(f\" the output of the untrained Neuron is: {new_output_neuron2}\")\n",
    "\n",
    "new_activated_neuron2_output = 1 / (1 + math.exp(-new_output_neuron2))\n",
    "print(new_activated_neuron2_output)\n",
    "\n",
    "new_output_neuron3 = new_activated_neuron1_output * neuron3_weight1 + new_activated_neuron2_output * neuron3_weight2 + neuron3_bias\n",
    "print(f\" the output of the untrained Neuron is: {new_output_neuron3}\")\n",
    "\n",
    "new_activated_neuron3_output = 1 / (1 + math.exp(-new_output_neuron3))\n",
    "print(new_activated_neuron3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New loss: 0.010015986196649925, old loss: 0.010011002650325403\n"
     ]
    }
   ],
   "source": [
    "new_loss = 0.5 * (new_activated_neuron3_output - expected_ouput) ** 2\n",
    "print(f\"New loss: {new_loss}, old loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for step in range(1000):\n",
    "    # Forward Pass\n",
    "    z = input1 * weight1 + input2 * weight2 + bias\n",
    "    nn_activated_output = 1 / (1 + math.exp(-z))\n",
    "\n",
    "    # Loss calculation (Mean Squared Error)\n",
    "    loss = 0.5 * (nn_activated_output - expected_ouput) ** 2\n",
    "\n",
    "    # Backward Pass\n",
    "    # Gradient of the loss with respect to the NN output\n",
    "    d_loss_d_output = nn_activated_output - expected_ouput\n",
    "\n",
    "    # Gradient of the NN output with respect to z\n",
    "    d_output_d_z = nn_activated_output * (1 - nn_activated_output)\n",
    "\n",
    "    # Combine gradients to get the gradient of the loss with respect to w1, w2, and bias\n",
    "    d_loss_d_w1 = d_loss_d_output * d_output_d_z * weight1\n",
    "    d_loss_d_w2 = d_loss_d_output * d_output_d_z * weight2\n",
    "    d_loss_d_bias = d_loss_d_output * d_output_d_z * bias\n",
    "\n",
    "    # Update weights and bias\n",
    "    weight1 -= learning_rate * d_loss_d_w1\n",
    "    weight2 -= learning_rate * d_loss_d_w2\n",
    "    bias -= learning_rate * d_loss_d_bias\n",
    "\n",
    "    # Print the loss every 100 epochs\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: Loss = {loss}\")\n",
    "\n",
    "print(f\"Final weight1: {weight1}, Final weight2: {weight2}, Final bias: {bias}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
