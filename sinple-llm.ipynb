{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will implement LLM from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "We will use pytorch and numpy for the AI, ML computations.\n",
    "Matplotlib for the visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gokdenizgulmez/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params\n",
    "Here are the configurations for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dimension = 8 # Also known as \"hidden_size\" in hugginface, the inner mlp dimension or \"intermediete_size\" is 8 (embeddings_dimension) * 4 = 32\n",
    "num_attention_heads = 2\n",
    "attention_head_size = embeddings_dimension // num_attention_heads # This is the dimension every attention head wil have\n",
    "num_transformer_blocks = 2\n",
    "max_context_lenght = 128 # also known as \"max_position_embeddings\" in hugginface, is the maximum of tokens the llm can process\n",
    "vocab_size = 0 # Set it to zero because it's going to update it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is only for creating the Plot to visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPlot(nums, xlabel = \"Embedding Dimensions\"):\n",
    "    # Convert embeddings to numpy for visualization\n",
    "    embeddings_np = nums.detach().numpy()\n",
    "\n",
    "    # Get dimensions\n",
    "    embeddings_dimension = embeddings_np.shape[1]\n",
    "    tokenized_input_length = embeddings_np.shape[0]\n",
    "    tokenized_input = list(range(tokenized_input_length))\n",
    "\n",
    "    # Plotting embeddings\n",
    "    fig, ax = plt.subplots(figsize=(25, 5))  # Increase figure size for bigger squares\n",
    "    cax = ax.matshow(embeddings_np, aspect='auto', cmap='viridis')\n",
    "\n",
    "    # Add color bar for reference\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set labels\n",
    "    ax.set_xticks(np.arange(embeddings_dimension))\n",
    "    ax.set_yticks(np.arange(tokenized_input_length))\n",
    "    ax.set_xticklabels([f'Dim {i}' for i in range(embeddings_dimension)])\n",
    "    ax.set_yticklabels([f'Token {i}' for i in tokenized_input])\n",
    "\n",
    "    # Rotate the tick labels and set their alignment\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Embedding Dimensions\")\n",
    "    plt.ylabel(\"labelpad=Tokens\")\n",
    "\n",
    "    # Adding numerical values to the plot\n",
    "    for i in range(tokenized_input_length):\n",
    "        for j in range(embeddings_dimension):\n",
    "            text = ax.text(j, i, f'{embeddings_np[i, j]:.2f}', ha='center', va='center', color='white')\n",
    "\n",
    "    plt.title(\"Embeddings Visualization\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "Here we will create the Tokenizer and the input/training text\n",
    "\n",
    "Where \"S\" is the Start of sequence Token\n",
    "Where \"E\" is the End of sequence Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"SThis is a input textE\" # Because its a character leverl tokenizer the SOS is: S and the EOS is: E\n",
    "\n",
    "# Here w create the Tokenizer\n",
    "chars = sorted(list(set(input_text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "tokenize = lambda s: [stoi[c] for c in s] # tokenizer: take a string, output a list of integers\n",
    "detokenize = lambda l: ''.join([itos[i] for i in l]) # detokenizer: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenize(input_text)\n",
    "eos_token = tokenized_input[-1]\n",
    "sos_token = tokenized_input[0]\n",
    "\n",
    "print(f\"eos_token: {eos_token}\")\n",
    "print(f\"sos_token: {sos_token}\")\n",
    "print(f\"The tokenized Tensor: {tokenized_input}\")\n",
    "print(f\"The context size of the current input text is: {len(tokenized_input)}\")\n",
    "print(f\"The detokenized Tensor: \\\"{detokenize(tokenized_input)}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_dimension, padding_idx=eos_token)\n",
    "embeddings = embedder(torch.tensor(tokenized_input))\n",
    "\n",
    "print(f\"The shape is: {embeddings.shape}\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to numpy for visualization\n",
    "createPlot(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_attention_layernorm = nn.LayerNorm(embeddings_dimension)\n",
    "attention_input = post_attention_layernorm(embeddings)\n",
    "\n",
    "print(attention_input.shape)\n",
    "createPlot(attention_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.key_proj = nn.Linear(embeddings_dimension, attention_head_size, bias=False)\n",
    "        self.query_proj = nn.Linear(embeddings_dimension, attention_head_size, bias=False)\n",
    "        self.value_proj = nn.Linear(embeddings_dimension, attention_head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(max_context_lenght, max_context_lenght)))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        L, D = x.shape\n",
    "\n",
    "        keys = self.key_proj(x)   # (B,T,hs)\n",
    "        print(\"keys beggining\")\n",
    "        createPlot(keys)\n",
    "        queries = self.query_proj(x) # (B,T,hs)\n",
    "        print(\"queries beggining\")\n",
    "        createPlot(queries)\n",
    "        values = self.value_proj(x) # (B,T,hs)\n",
    "        print(\"values beggining\")\n",
    "        createPlot(values)\n",
    "        \n",
    "        # ATTENTION START\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = queries @ keys.transpose(-2,-1) * keys.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        print(\"keys and queries multiplied\")\n",
    "        createPlot(wei)\n",
    "        # Mask the input\n",
    "        wei = wei.masked_fill(self.tril[:L, :L] == 0, float('-inf')) # (B, T, T)\n",
    "        print(\"Masked\")\n",
    "        createPlot(wei)\n",
    "\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        print(\"Softmaxed\")\n",
    "        createPlot(wei)\n",
    "        wei = self.dropout(wei)\n",
    "        print(\"droped out\")\n",
    "        createPlot(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        out = wei @ values # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        print(\"Attentino head output\")\n",
    "        createPlot(out)\n",
    "        # ATTENTION END\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead() for _ in range(num_attention_heads)])\n",
    "        self.out_proj = nn.Linear(attention_head_size * num_attention_heads, embeddings_dimension, bias=False)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        print(\"concatinated attention outputs\")\n",
    "        createPlot(out)\n",
    "        print(\"final output multihaead attention\")\n",
    "        out = self.out_proj(out)\n",
    "        createPlot(out)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention()\n",
    "print(mha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_output = mha(embeddings)\n",
    "print(mha_output.shape)\n",
    "print(mha_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gate_proj = nn.Linear(embeddings_dimension, 4 * embeddings_dimension, bias=False)\n",
    "        self.down_proj = nn.Linear(4 * embeddings_dimension, embeddings_dimension, bias=False)\n",
    "        self.up_proj = nn.Linear(embeddings_dimension, 4 * embeddings_dimension, bias=False)\n",
    "\n",
    "        self.act_fn = nn.SELU()\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        print(\"gate_proj\")\n",
    "        createPlot(gate)\n",
    "\n",
    "        up = self.up_proj(x)\n",
    "        print(\"up_proj\")\n",
    "        createPlot(up)\n",
    "\n",
    "        x = self.act_fn(gate * up)\n",
    "        print(\"multiplied and activated\")\n",
    "        createPlot(x)\n",
    "\n",
    "        down = self.down_proj(x)\n",
    "        print(\"down_proj\")\n",
    "        createPlot(down)\n",
    "        return down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_output = mlp(mha_output)\n",
    "\n",
    "print(mlp_output.shape)\n",
    "print(mlp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.mlp = MLP()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embeddings_dimension)\n",
    "        self.ln2 = nn.LayerNorm(embeddings_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_o = self.attn(self.ln1(x))\n",
    "        print(\"Transformer Block attn output:\")\n",
    "        createPlot(attn_o)\n",
    "\n",
    "        x = x + attn_o\n",
    "        print(\"Transformer Block first Residual Conection:\")\n",
    "        createPlot(x)\n",
    "\n",
    "        mlp_o = self.mlp(self.ln2(x))\n",
    "        print(\"Transformer Block mlp output:\")\n",
    "        createPlot(mlp_o)\n",
    "\n",
    "        x = x + mlp_o\n",
    "        print(\"Transformer Block second Residual Conection and final Output:\")\n",
    "        createPlot(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_block = TransformerBlock()\n",
    "print(transformer_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_block(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerBlock() for _ in range(num_transformer_blocks)])\n",
    "        self.ln_f = nn.LayerNorm(embeddings_dimension)\n",
    "        self.lm_head = nn.Linear(embeddings_dimension, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.transformer_blocks(x) # (B,T,C)\n",
    "        print(f\"output transformer blocks:\")\n",
    "        createPlot(x)\n",
    "\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        print(f\"layer norm:\")\n",
    "        createPlot(x)\n",
    "\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        print(f\"output lm heads (logits):\")\n",
    "        createPlot(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass the created embeddings through the Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = model(embeddings)\n",
    "\n",
    "# so the outut of the transformer is the shape torch.Size([22, 14]) the input was torch.Size([22, 13]) the last layer in the shape is the next generated token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits)\n",
    "print(loss) # Because its not training it will output \"None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape)\n",
    "\n",
    "# focus only on the last time step eg the last embeddings layer\n",
    "last_logits = logits[-1, :] # becomes (D)\n",
    "\n",
    "print(last_logits)\n",
    "createPlot(last_logits)\n",
    "\n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(last_logits, dim=-1) # (B, C)\n",
    "\n",
    "# sample from the distribution\n",
    "next_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detokenize the generated output Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detokenize(next_token.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "\n",
    "print(logits.shape)\n",
    "\n",
    "for i in range(10):\n",
    "    # focus only on the last time step eg the last embeddings layer\n",
    "    last_logits = logits[-1, :] # becomes (D)\n",
    "    print(f\"last logit embeddings{last_logits}\")\n",
    "    # apply softmax to get probabilities\n",
    "    probs = F.softmax(last_logits, dim=-1) # (B, C)\n",
    "    # sample from the distribution\n",
    "    next_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "    all_tokens.append(next_token.item())\n",
    "    print(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_tokens)\n",
    "print(detokenize(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.key_proj = nn.Linear(embeddings_dimension, attention_head_size, bias=False)\n",
    "        self.query_proj = nn.Linear(embeddings_dimension, attention_head_size, bias=False)\n",
    "        self.value_proj = nn.Linear(embeddings_dimension, attention_head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(max_context_lenght, max_context_lenght)))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        L, _ = x.shape\n",
    "\n",
    "        keys = self.key_proj(x)   # (B,T,hs)\n",
    "        queries = self.query_proj(x) # (B,T,hs)\n",
    "        values = self.value_proj(x) # (B,T,hs)\n",
    "        \n",
    "        # ATTENTION START\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = queries @ keys.transpose(-2,-1) * keys.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:L, :L] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        out = wei @ values # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        # ATTENTION END\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead() for _ in range(num_attention_heads)])\n",
    "        self.out_proj = nn.Linear(attention_head_size * num_attention_heads, embeddings_dimension, bias=False)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.out_proj(out)\n",
    "        return self.dropout(out)\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gate_proj = nn.Linear(embeddings_dimension, 4 * embeddings_dimension, bias=False)\n",
    "        self.down_proj = nn.Linear(4 * embeddings_dimension, embeddings_dimension, bias=False)\n",
    "        self.up_proj = nn.Linear(embeddings_dimension, 4 * embeddings_dimension, bias=False)\n",
    "        self.act_fn = nn.SELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x) * self.up_proj(x)))\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.mlp = MLP()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embeddings_dimension)\n",
    "        self.ln2 = nn.LayerNorm(embeddings_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_dimension, padding_idx=eos_token)\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerBlock() for _ in range(num_transformer_blocks)])\n",
    "        self.ln_f = nn.LayerNorm(embeddings_dimension)\n",
    "        self.lm_head = nn.Linear(embeddings_dimension, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.transformer_blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last max_context_size tokens\n",
    "            idx_cond = idx[:, -max_context_lenght:]\n",
    "            # get the predictions\n",
    "            new_logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            new_logits = new_logits[-1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(new_logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = LlamaModel()\n",
    "print(sum(p.numel() for p in llama.parameters())/1e6, 'M parameters')\n",
    "print(llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = llama.generate(torch.tensor(tokenized_input), max_new_tokens=5)[0].tolist()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
