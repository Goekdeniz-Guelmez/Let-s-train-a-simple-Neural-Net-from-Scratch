{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library\n",
    "import math\n",
    "\n",
    "# Set initial values\n",
    "x1 = 1\n",
    "x2 = 0\n",
    "w1_1 = 0.4\n",
    "w1_2 = -1.5\n",
    "w2_1 = 0.3\n",
    "w2_2 = -0.5\n",
    "bias1 = 0.4\n",
    "bias2 = 0.2\n",
    "learning_rate = 0.1\n",
    "original_output1 = 0\n",
    "original_output2 = 1\n",
    "epochs = 1000  # Number of training iterations\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "# Derivative of sigmoid function\n",
    "def sigmoid_derivative(output):\n",
    "    return output * (1 - output)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass for output 1\n",
    "    z1 = x1 * w1_1 + x2 * w1_2 + bias1\n",
    "    nn_output1 = sigmoid(z1)\n",
    "    \n",
    "    # Forward Pass for output 2\n",
    "    z2 = x1 * w2_1 + x2 * w2_2 + bias2\n",
    "    nn_output2 = sigmoid(z2)\n",
    "\n",
    "    # Loss calculation (Mean Squared Error)\n",
    "    loss1 = 0.5 * (nn_output1 - original_output1) ** 2\n",
    "    loss2 = 0.5 * (nn_output2 - original_output2) ** 2\n",
    "    total_loss = loss1 + loss2\n",
    "\n",
    "    # Backward Pass for output 1\n",
    "    d_loss1_d_output = nn_output1 - original_output1\n",
    "    d_output_d_z1 = sigmoid_derivative(nn_output1)\n",
    "    d_z_d_w1_1 = x1\n",
    "    d_z_d_w1_2 = x2\n",
    "    d_z_d_bias1 = 1\n",
    "    d_loss1_d_w1_1 = d_loss1_d_output * d_output_d_z1 * d_z_d_w1_1\n",
    "    d_loss1_d_w1_2 = d_loss1_d_output * d_output_d_z1 * d_z_d_w1_2\n",
    "    d_loss1_d_bias1 = d_loss1_d_output * d_output_d_z1 * d_z_d_bias1\n",
    "\n",
    "    # Backward Pass for output 2\n",
    "    d_loss2_d_output = nn_output2 - original_output2\n",
    "    d_output_d_z2 = sigmoid_derivative(nn_output2)\n",
    "    d_z_d_w2_1 = x1\n",
    "    d_z_d_w2_2 = x2\n",
    "    d_z_d_bias2 = 1\n",
    "    d_loss2_d_w2_1 = d_loss2_d_output * d_output_d_z2 * d_z_d_w2_1\n",
    "    d_loss2_d_w2_2 = d_loss2_d_output * d_output_d_z2 * d_z_d_w2_2\n",
    "    d_loss2_d_bias2 = d_loss2_d_output * d_output_d_z2 * d_z_d_bias2\n",
    "\n",
    "    # Update weights and biases\n",
    "    w1_1 -= learning_rate * d_loss1_d_w1_1\n",
    "    w1_2 -= learning_rate * d_loss1_d_w1_2\n",
    "    bias1 -= learning_rate * d_loss1_d_bias1\n",
    "    w2_1 -= learning_rate * d_loss2_d_w2_1\n",
    "    w2_2 -= learning_rate * d_loss2_d_w2_2\n",
    "    bias2 -= learning_rate * d_loss2_d_bias2\n",
    "\n",
    "    # Print the loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Total Loss = {total_loss}\")\n",
    "\n",
    "# Print final updated parameters\n",
    "print(f\"Updated w1_1: {w1_1}\")\n",
    "print(f\"Updated w1_2: {w1_2}\")\n",
    "print(f\"Updated bias1: {bias1}\")\n",
    "print(f\"Updated w2_1: {w2_1}\")\n",
    "print(f\"Updated w2_2: {w2_2}\")\n",
    "print(f\"Updated bias2: {bias2}\")\n",
    "\n",
    "# Print the final outputs for verification\n",
    "print(f\"Final NN Output1: {nn_output1}\")\n",
    "print(f\"Final NN Output2: {nn_output2}\")\n",
    "print(f\"Final Total Loss: {total_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
