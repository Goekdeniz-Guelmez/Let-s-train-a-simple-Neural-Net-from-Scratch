{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "<p>We will train a single Neuron with one input and output.</p>\n",
    "<p>For the harder calculations, we need to imoprt the math package in python.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by defining the learning rate. the learning rate is the amout of changes in out Bacpropogation we allow, the smaller the number, the longer the training takes but also the more acurate the predictions are going to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's start by defining our input and output. This is our dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = 1\n",
    "input2 = 0\n",
    "expected_ouput = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Neuron\n",
    "\n",
    "<p>The first time a Neuron is created, it gets a randomly assigned values for its weight and bias.</p>\n",
    "<p>The number of weights is the same as the inputs and every Neuron get's one bias, you can also chose to leave the bias out. So therefore, whe need two weights and one bias.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = 0.4\n",
    "weight2 = -1.2\n",
    "bias = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propogation\n",
    "\n",
    "<p>After creating the Neuron we can go ahead and Forward Pass through the model.</p>\n",
    "<p>We are going to compute the weightes sum of the inputs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_neuron = input1 * weight1 + input2 * weight2 + bias\n",
    "print(f\" the output of the untrained Neuron is: {output_neuron}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run that output through the Activation Function to finish the Forward Pass. the Functin we'll use is the Sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activated_neuron_output = 1 / (1 + math.exp(-output_neuron))\n",
    "print(activated_neuron_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss calculation\n",
    "\n",
    "<p>Perfect! We are done with the Forward Propogation. We can now calculate the loss. The Algorythm we will be using is the \"Mean Square Error\".</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0.5 * (activated_neuron_output - expected_ouput) ** 2\n",
    "print(f\"The first loss is: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propogation\n",
    "\n",
    "<p>Now we got everything setup and can procede with the Backward Pass. And the first thing we need to do is to calculate the gradients.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of the loss with respect to the NN output\n",
    "d_loss_d_output = activated_neuron_output - expected_ouput\n",
    "print(f\"Gradient of the loss with respect to the NN output: {d_loss_d_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can use the derivative to calculate this perticular gradient.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of the NN output with respect to activated_neuron_output\n",
    "d_output_d_z = activated_neuron_output * (1 - activated_neuron_output)\n",
    "print(f\"Gradient of the NN output with respect to nn_output is: {d_output_d_z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's combine gradients to get the gradient of the loss with respect to w1, w2, and bias\n",
    "d_loss_d_w1 = loss * d_output_d_z * weight1\n",
    "d_loss_d_w2 = loss * d_output_d_z * weight2\n",
    "d_loss_d_bias = loss * d_output_d_z * bias\n",
    "\n",
    "print(f\"gradient of Weight1: {d_loss_d_w1}, gradient of Weight2: {d_loss_d_w2}, gradient of the bias: {d_loss_d_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are all set up to update our weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights and bias\n",
    "# We first multiply the learning rate with the gradients abouve and then subtract that value with the origional paramters.\n",
    "weight1 -= learning_rate * d_loss_d_w1\n",
    "weight2 -= learning_rate * d_loss_d_w2\n",
    "bias -= learning_rate * d_loss_d_bias\n",
    "\n",
    "print(f\"New weight1: {weight1}, New weight2: {weight2}, New bias: {bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have our new Weights and our new bias, let's forward Propogate through the same Neuron with our new paramters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output_neuron = input1 * weight1 + input2 * weight2 + bias\n",
    "print(f\" the output of the new Neuron is: {new_output_neuron}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_activated_neuron_output = 1 / (1 + math.exp(-new_output_neuron))\n",
    "print(new_activated_neuron_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To test if we traind our Neiron corectly well calculate the loss and if its smaller, we have succesfully trained the Neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loss = 0.5 * (new_activated_neuron_output - expected_ouput) ** 2\n",
    "print(f\"New loss: {new_loss}, old loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratiolations we traind out forst Neural Neuron\n",
    "\n",
    "<p>Let's repeat that for 10 Steps.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for step in range(1000):\n",
    "    # Forward Pass\n",
    "    z = input1 * weight1 + input2 * weight2 + bias\n",
    "    nn_activated_output = 1 / (1 + math.exp(-z))\n",
    "\n",
    "    # Loss calculation (Mean Squared Error)\n",
    "    loss = 0.5 * (nn_activated_output - expected_ouput) ** 2\n",
    "\n",
    "    # Backward Pass\n",
    "    # Gradient of the loss with respect to the NN output\n",
    "    d_loss_d_output = nn_activated_output - expected_ouput\n",
    "\n",
    "    # Gradient of the NN output with respect to z\n",
    "    d_output_d_z = nn_activated_output * (1 - nn_activated_output)\n",
    "\n",
    "    # Combine gradients to get the gradient of the loss with respect to w1, w2, and bias\n",
    "    d_loss_d_w1 = d_loss_d_output * d_output_d_z * weight1\n",
    "    d_loss_d_w2 = d_loss_d_output * d_output_d_z * weight2\n",
    "    d_loss_d_bias = d_loss_d_output * d_output_d_z * bias\n",
    "\n",
    "    # Update weights and bias\n",
    "    weight1 -= learning_rate * d_loss_d_w1\n",
    "    weight2 -= learning_rate * d_loss_d_w2\n",
    "    bias -= learning_rate * d_loss_d_bias\n",
    "\n",
    "    # Print the loss every 100 epochs\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: Loss = {loss}\")\n",
    "\n",
    "print(f\"Final weight1: {weight1}, Final weight2: {weight2}, Final bias: {bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can see the loss getting smaller and smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
